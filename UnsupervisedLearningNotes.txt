Unsupervised Learning Techniques:
There are two popular categories of unsupervised learning techniques:
- Clustering: Identifying groups (or clusters) of data points that are similar to one another but distinct from other groups.
	Common Techniques:
	- K-Means Clustering.
	- Hierarchical Clustering.
	- DBSCAN (Density-Based Clustering)
	
	Applications:
	- Clustering / Segmentation.
	- Anomaly Detection. Identiying rare points in a data set that deviate significantly from the rest.
		- Clustering Techniques.
		- Isolation Forests.
	- Recommenders.Suggesting items to users based on their preferences or behaviors.
		- Clustering Techniques
		- Dimensionality Reduction Techniques.

- Dimensionality Reduction: Reducing the number of columns (or dimensions) in a data set while losing as little information as possible.
	Common Techniques:
	- PCA (Principal Component Analysis)
	- t-SNE (t-Stochastic Neighbor Embedding)
	- SVD (Singular Value Decomposition)

	Applications:
	- Feature Extraction.
	- Data Visualization.
	- Recommenders.

Data Science Workflow. This is non-linear
1) Scoping a project
2) Gathering data
3) Cleaning data
4) Exploring data
5) Modeling data
6) Sharing insights

Unsupervised Learning Workflow: This is also non-linear from step 1,2 and 3.
1) Data Prep: Get your data ready to be input into a model.
	- Single table, non-null, numeric data.
	- Feature engineering, selection, and scaling.
2) Algorithm: Apply an unsupervised learning technique.
	- Clustering.
	- Dimensinality Reduction.
3) Tuning: Evaluate & tune the model using metrics and intuition.
	- Metrics (i.e. inertia)
	- Data visualization.
	- Interpret the results.
4) Selection: Pick the best results & identify any insights.
	- Business objective.
	- Domain expertise.

------------------------------------------------------------------------------------------------
The general clustering workflow:
1) Data Prep: Get data ready to be input into an ML model
	- Single table, non-null values and numeric data.
	- Feature engineering, selection and scaling.
2) Modeling: Apply a cluster algorithm.
	- K-Means clustering
	- Hierarchical Clustering
	- DBSCAN
3) Tuning: Evaluate & tune the model using metrics and intuition.
	- Metrics (i.e. inertia)
	- Data Visualization
	- Interpret the results
4) Selection: Pick the best result and identify any insights.
	- Business objective
	- Domain expertise

------------------------------------------------------------------------------------------------
Anomaly Detection  (Also known as Outlier Detection)
Used to identify observations in a data set that are significantly different from the others.
Anomaly detection happens either in the cleaning data step, modeling data step or both.
Approaches:
- Statistical analysis: flagothan 3 standard deviations away from the mean.
- Data visualization: visually identifying distant points by creating histograms, scatterplots, or boxplots.
- Supervised learning: if you have information on past anomalies, you can apply classification algorithms like Random Forests to identify new ones.
- Unsupervised learning: if you have unlabeled data, you need algorithms like Isolation Forests and DBSCAN
[Many machine learning algorithms can be modified for anomaly detection including K-Nearest Neighbors, One-Class Support Vector Machines, Time Series Analysis, etc.]

Anomaly Detection Workflow:
1) Data Prep: Get your data ready to be input into an ML model
    - Single table, non-null and numeric data.
    - Feature engineering, selection and scaling.
2) Modeling: Apply an algorithm for anomaly detection.
    - Isolation Forests
    - DBSCAN
3) Tuning: Evaluate and tune the model using plots and intuition.
    - Data visualization.
    - Interpret the results.
4) Selection: Pick the best results and identify any insights.
    - Business objective.
    - Domain expertise.

------------------------------------------------------------------------------------------------
Dimensionality Reduction
Involves reducing the number of columns (dimensions) in a data set while losing as little information as possible.
Reducing the numbers of dimensions in a data set helps with these issues:
- EDA: It's difficult for humans to visualize data with more than 2 or 3 dimensions clearly.
- Supervised Learning: Predictive models fit on "wide" data don't perform as well as those fit on "long" data.
- Unsupervised Learning: Data points spread out with more dimensions, and their distances become more similar (curse of dimensionality)

Algorithms:
- PCA (Principal Component Analysis): is a dimensionality reduction technique that finds linear combinations of features that explain the most variation in the data.

- t-SNE






















